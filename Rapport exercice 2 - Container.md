
---

Prof : Charlie Bromberg
Groupe 08 : Owen Jeammot, MaÃ«l Joumier

---
**Objectif de l'exercice :**
- Deployer un cluster k8s en utilisant minikube
- Configurer les services du cluster en applicant les concepts vu en cours
- Introduire des vulnerabilites intentionnelles permettant un chemin d'attaque precis
- Exploiter le cluster en appliquant les principes d'attaque et d'escalade

**Scenario propose :**
Exposer un server web (ici flask) qui est vulnÃ©rable a l'injection de commande -> Escalade de privilege sur la machine web via un sudo NOPASSWD sur python -> DÃ©couverte reseau afin de trouver un pods non accessible depuis l'exterieurs -> Decouverte d'un service SSH accessible en connexion root avec un mot de passe faible -> Escape du pod  qui etait alors privileged -> Prise totale du cluster.

On peut retrouver ci dessous le schÃ©ma explicatif permettant de rÃ©sumer l'attaque.


<p align="center">
  <img src="image-removebg-preview.png" alt="description">
</p>

## Mise en place du cluster

> OS : Debian
>Il faut au prÃ©alable avoir installer [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/) et [minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fdebian+package)

Voici la structure du projet : 
```
ğŸ“ exo-2-k8s
â”œâ”€â”€ ğŸ“ docker
â”‚   â”œâ”€â”€ ğŸ app.py
â”‚   â””â”€â”€ ğŸ³ Dockerfile
â”œâ”€â”€ ğŸ“ exploit
â”‚   â”œâ”€â”€ ğŸ“„ cmd.txt
â”‚   â””â”€â”€ ğŸ exploit.py
â”œâ”€â”€ ğŸ“ k8s
â”‚   â”œâ”€â”€ ğŸ“ settings
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ namespaces.yaml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ networkpolicy.yaml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ rbac.yaml
â”‚   â”‚   â””â”€â”€ ğŸ“„ volumes.yaml
â”‚   â”œâ”€â”€ ğŸ“ ssh
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ vuln-ssh-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ vuln-ssh-secret.yaml
â”‚   â”‚   â””â”€â”€ ğŸ“„ vuln-ssh-service.yaml
â”‚   â””â”€â”€ ğŸ“ web
â”‚       â”œâ”€â”€ ğŸ“„ vuln-web-configmap.yaml
â”‚       â”œâ”€â”€ ğŸ“„ vuln-web-deployment.yaml
â”‚       â”œâ”€â”€ ğŸ“„ vuln-web-service.yaml
â”œâ”€â”€ âš™ï¸ .gitignore
â”œâ”€â”€ ğŸ“œ LICENSE
â””â”€â”€ ğŸ“ README.md

```

Pour commencer nous allons nous focaliser sur la partie du server web en flask. Nous avons vraiment fait un server minimaliste qui permet simplement de lancer des commande sur le docker via une request avec l'argument cmd.

```python
from flask import Flask, request
import os

app = Flask(__name__)

@app.route('/ping')
def ping():
Â  Â  cmd = request.args.get('cmd')
Â  Â  if cmd:
Â  Â  Â  Â  return os.popen(cmd).read()
Â  Â  return "OK"

app.run(host="0.0.0.0", port=5000)
```

Une fois notre server simple teste nous pouvons mettre le projet dans un `Dockerfile`

```docker
FROM python:3.13.3-bookworm

# Update et installation de sudo
RUN apt update && apt install -y sudo

# Ajout des droits sudo en nopasswd au binaire python3
RUN echo "ALL ALL=(ALL) NOPASSWD: /usr/local/bin/python3" >> /etc/sudoers

# Creation de l'user flaskuser
RUN useradd -m -s /bin/bash flaskuser

# Installation du module flask
RUN pip install flask

# Copie du script python app.py en local vers notre docker
COPY app.py /app/app.py

# Initialisation du directory initiale
WORKDIR /app

# Setup de l'user par default du docker
USER flaskuser

# Commande de lancement du script python
CMD ["python", "app.py"]
```
Nous venons de crere notre application web dans un docker nous pouvons maintenant nous lancer dans la construction de notre cluster minikube. Pour ce faire on dÃ©marre minikube : `minikube start`. Puis on va pouvoir build notre image docker web que l'on vient de crerer dans le cluster minikube :
```bash
eval $(minikube docker-env)
docker build -t vuln-web:latest .
```
A note qu'il est important de faire un eval pour changer le contexte dans le quel on veut build le docker sinon ce dernier va se build dans notre registry local et pas sur le cluster minikube.
Voila parfait nous avons tout les prÃ©requis afin de construire notre projet, lancons-nous maintenant dans la crÃ©ation de notre cluster. Pour ce faire nous allons commencer par crÃ©er tout nos fichiers de configuration afin de crÃ©er le cluster.

### K8S cluster settings :

On commence par se focusse sur cette partie : 
```
â”œâ”€â”€ ğŸ“ k8s
â”‚   â”œâ”€â”€ ğŸ“ settings
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ namespaces.yaml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ networkpolicy.yaml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ rbac.yaml
â”‚   â”‚   â””â”€â”€ ğŸ“„ volumes.yaml
```
##### **Namespaces :**

```yml
---
apiVersion: v1
kind: Namespace
metadata:
Â  name: vuln-web-external

---
apiVersion: v1
kind: Namespace
metadata:
Â  name: vuln-ssh-internal
```

Assez simple ici on cree deux namespace : vuln-web-external et vuln-ssh-internal. Le but d'un namespace est de separer les ressources dans des espaces logique differents afin de mieux gere les ressources.

##### **Networkpolicy :**

```yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
Â  name: allow-web-to-ssh
Â  namespace: vuln-ssh-internal
spec:
Â  podSelector: # permet de selectionner le pods vise par la regle
Â  Â  matchLabels:
Â  Â  Â  app: vuln-ssh
Â  ingress:
Â  - from:
Â  Â  - namespaceSelector: # permet d'autoriser l'access au namespace vuln-web-external
Â  Â  Â  Â  matchLabels:
Â  Â  Â  Â  Â  name: vuln-web-external
Â  policyTypes:
Â  - Ingress
```

Le but de ce fichier de configuration est de restrindre l'access a la machine vuln-ssh seulement au pods qui appartienne au namespace vuln-web-external. Le but des NetworkPolicy sont de creer des regles pour la connectivite des ressources dans le cluster.

##### **RBAC :**

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
Â  name: ssh-admin
Â  namespace: vuln-ssh-internal
Â  
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
Â  name: superadmin
rules:
- apiGroups: ["*"]
Â  resources: ["*"]
Â  verbs: ["*"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
Â  name: superadmin-binding
subjects:
- kind: ServiceAccount
Â  name: ssh-admin
Â  namespace: vuln-ssh-internal
roleRef:
Â  kind: ClusterRole
Â  name: superadmin
Â  apiGroup: rbac.authorization.k8s.io
```

RBAC (Role-Based Access Control), ce dernier permet de donner des droits administrateurs a un pod ou une personne, un processus... Dans ce fichier on s'occupe donc de faire 3 chose : 
	- Creation d'un compte de service `ssh-admin` dans le namespace `vuln-ssh-internal`
	- Creation d'un role rbac qui donne tout les droits sur tout les groupes d'apim toutes les ressources et tout les verbs.
	- Creation du lien entre le role rbac et le compte de service.
Grace a ces trois regles on a maintenant un compte de service `ssh-admin` qui possede tout les droits sur le cluster.


##### **Volumes :**

```yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
Â  name: host-ssh-pv
spec:
Â  capacity:
Â  Â  storage: 10Gi
Â  accessModes:
Â  Â  - ReadWriteOnce
Â  persistentVolumeReclaimPolicy: Retain
Â  hostPath:
Â  Â  path: /

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
Â  name: host-ssh-pvc
Â  namespace: vuln-ssh-internal
spec:
Â  accessModes:
Â  Â  - ReadWriteOnce
Â  resources:
Â  Â  requests:
Â  Â  Â  storage: 1Gi
```

On s'occupe ici de creer un PV (PersistentVolume) et  un PVC (PersistentVolumeClaim).
	- PV : C'est un espace de stockage dans le cluster mis Ã  disposition par un administrateur ou provisionnÃ© automatiquement. Câ€™est comme un disque dur partagÃ©, que Kubernetes peut connecter Ã  des pods.
	- PVC est une demande de stockage faite par un pod qui permet de dire combien il veut en espace, les droits (R, W...).
On a donc dans ce fichier de configuration la creation d'un volume de 10G en RW qui est monte sur `/` de l'host. Puis on a une demande d'attribution d'espace dans le namespace `vuln-ssh-internal` de 1G.

Voila avec tout cela nous avons fini de configurer les parametteres "generale" de notre cluster et nous allons pouvoir nous focus sur la creation de nos services. Commencons par le service web expose : 

### K8S cluster web :

```
â”‚   â””â”€â”€ ğŸ“ web
â”‚       â”œâ”€â”€ ğŸ“„ vuln-web-configmap.yaml
â”‚       â”œâ”€â”€ ğŸ“„ vuln-web-deployment.yaml
â”‚       â”œâ”€â”€ ğŸ“„ vuln-web-service.yaml
```

Le but ici va donc de presenter les differents fichiers de configuration necessaire a la creation du pods web dans les conditions necessaire a notre scenario :

##### **ConfigMap :**

```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
Â  name: vuln-web-config
Â  namespace: vuln-web-external
data:
Â  FLASK_ENV: "development"
```

Un configmap permet de stocker des parametres de configuration pour des applciations qui sont dans des pods. On peut y mettre des variables d'enriroemment comme ici : `FLASK_ENV: "development"` mais on peut aussi mettre des fichiers de configuration, des chaines de texte ... Tout ca dans le but d'externatliser la config pour pouvoir modifier le comporatement du pods sans changer l'image docker.

##### **Deployment :**


```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
Â  name: vuln-web
Â  namespace: vuln-web-external
spec:
Â  replicas: 1
Â  selector:
Â  Â  matchLabels:
Â  Â  Â  app: vuln-web
Â  template:
Â  Â  metadata:
Â  Â  Â  labels:
Â  Â  Â  Â  app: vuln-web
Â  Â  spec:
Â  Â  Â  containers:
Â  Â  Â  - name: web
Â  Â  Â  Â  image: vuln-web:latest
Â  Â  Â  Â  imagePullPolicy: Never # permet de prendre l'image build en local precedement
Â  Â  Â  Â  resources: # permet de preciser la consomation max du pod
Â  Â  Â  Â  Â  limits:
Â  Â  Â  Â  Â  Â  memory: 512Mi
Â  Â  Â  Â  Â  Â  cpu: "1"
Â  Â  Â  Â  Â  requests:
Â  Â  Â  Â  Â  Â  memory: 256Mi
Â  Â  Â  Â  Â  Â  cpu: "0.2"
Â  Â  Â  Â  ports:
Â  Â  Â  Â  - containerPort: 5000
Â  Â  Â  Â  livenessProbe: # Permet de verifier que l'application est en vie
Â  Â  Â  Â  Â  httpGet:
Â  Â  Â  Â  Â  Â  path: /ping
Â  Â  Â  Â  Â  Â  port: 5000
Â  Â  Â  Â  Â  initialDelaySeconds: 5
Â  Â  Â  Â  Â  periodSeconds: 10
Â  Â  Â  Â  readinessProbe: # Permet de verifier que l'appl est prete a recevoir du trafic
Â  Â  Â  Â  Â  httpGet:
Â  Â  Â  Â  Â  Â  path: /ping
Â  Â  Â  Â  Â  Â  port: 5000
Â  Â  Â  Â  Â  initialDelaySeconds: 5
Â  Â  Â  Â  Â  periodSeconds: 10
Â  Â  Â  Â  envFrom: # Provisionement de variable d'envormmement en prenant la configmap
Â  Â  Â  Â  - configMapRef:
Â  Â  Â  Â  Â  Â  name: vuln-web-config
```

Ce fichier de type deployment permet de decrire precisement comment nous voulons deployer notre machine. Dans ce fichier on precise l'image docker que l'on veut utiliser, le nombre de replique que nous voulons, le label dans le quel on veut notre pods, les ressources que l'on veut q;il utilise...

##### **Service :**

```yaml
---
apiVersion: v1
kind: Service
metadata:
Â  name: vuln-web-svc
Â  namespace: vuln-web-external
spec:
Â  selector:
Â  Â  app: vuln-web
Â  ports:
Â  Â  - protocol: TCP
Â  Â  Â  port: 80 # Port interne dans le cluster
Â  Â  Â  targetPort: 5000 # Port du docker
Â  Â  Â  nodePort: 30080 # Port exterieur 
Â  type: NodePort
```

Le but de ce fichier de conguration de type `service` est d'exposer notre pods vers l'exterieur du cluster. Pour ce faire nous allons mapper le port originale du pods `5000` vers un port externieur sur notre cluster minikube `30080` afin de pouvoir tapper notre application depuis l'exterieur. Ici le service que l'on declare est de type `NodePort` nous permet de rendre l'applkication accessible sur : `http://IP_du_Node:30080`

Superbe maintenant notre application web vulnerable devrait pouvoir etre accesible depuis l'exterieure nous allons maintenant nous occuper de notre deuxieme machine vulnerable 

### K8S cluster ssh :

```
â”‚   â”œâ”€â”€ ğŸ“ ssh
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ vuln-ssh-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ vuln-ssh-secret.yaml
â”‚   â”‚   â””â”€â”€ ğŸ“„ vuln-ssh-service.yaml
```

Nous observons deja que la structure est assez similaire a notre precedent deploiyement de web. Plongons nous un en plus en profondeur afin de voir les diferences.

##### **Secret :**

```yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: ssh-credentials
  namespace: vuln-ssh-internal
type: kubernetes.io/basic-auth # Permet de provisionner des identifients
data:
  username: cm9vdA==        # "root"
  password: cm9vdA==        # "root"
```

A la difference de tout a l'heure au lieu de declarer une configMap nous avons declarer un fichier de configuration `Secret` qui comme son nom l'indique de gerer les secrets. Dans ce fichier nous utilisons donc le type `basic-auth` afin de provisionner des identifiants mot de passe a un compte.

##### **Deployment :**

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
Â  name: vuln-ssh
Â  namespace: vuln-ssh-internal
spec:
Â  replicas: 1
Â  selector:
Â  Â  matchLabels:
Â  Â  Â  app: vuln-ssh
Â  template:
Â  Â  metadata:
Â  Â  Â  labels:
Â  Â  Â  Â  app: vuln-ssh
Â  Â  spec:
Â  Â  Â  serviceAccountName: ssh-admin # Lui donne les acces ssh-admin
Â  Â  Â  containers:
Â  Â  Â  - name: vuln-ssh
Â  Â  Â  Â  image: rastasheep/ubuntu-sshd:18.04 # Image docker
Â  Â  Â  Â  imagePullPolicy: Always
Â  Â  Â  Â  resources: # Limitation materiel du pod
Â  Â  Â  Â  Â  limits:
Â  Â  Â  Â  Â  Â  memory: 512Mi
Â  Â  Â  Â  Â  Â  cpu: "1"
Â  Â  Â  Â  Â  requests:
Â  Â  Â  Â  Â  Â  memory: 256Mi
Â  Â  Â  Â  Â  Â  cpu: "0.2"
Â  Â  Â  Â  ports:
Â  Â  Â  Â  - containerPort: 22
Â  Â  Â  Â  securityContext:
Â  Â  Â  Â  Â  privileged: true # Permet l'acces au system host
Â  Â  Â  Â  Â  capabilities:
Â  Â  Â  Â  Â  Â  add: ["SYS_ADMIN", "SYS_PTRACE", "NET_ADMIN" # Donne des capabilities (droits).
Â  Â  Â  Â  Â  runAsUser: 0 # S'execute en root
Â  Â  Â  Â  Â  allowPrivilegeEscalation: true
Â  Â  Â  Â  env:
Â  Â  Â  Â  - name: ROOT_PASSWORD
Â  Â  Â  Â  Â  valueFrom:
Â  Â  Â  Â  Â  Â  secretKeyRef:
Â  Â  Â  Â  Â  Â  Â  name: ssh-credentials # Provisionnement du secret precedement evoque
Â  Â  Â  Â  Â  Â  Â  key: password
Â  Â  Â  Â  command: ["/bin/bash"]
Â  Â  Â  Â  args:
Â  Â  Â  Â  - "-c"
Â  Â  Â  Â  - |
Â  Â  Â  Â  Â  echo "root:$ROOT_PASSWORD" | chpasswd && \
Â  Â  Â  Â  Â  /usr/sbin/sshd -D # Commande au demarage du docker afin de mettre le mp du docker provisionner dans le secret
Â  Â  Â  Â  volumeMounts:
Â  Â  Â  Â  - name: host-root
Â  Â  Â  Â  Â  mountPath: /host 
Â  Â  Â  volumes: # Permet de monter le systeme de 
Â  Â  Â  - name: host-root
Â  Â  Â  Â  hostPath:
Â  Â  Â  Â  Â  path: /
Â  Â  Â  Â  Â  type: Directory
Â  Â  Â  restartPolicy: Always
```